# Awesome Multimodal Learning

<p align="center">
  <img width="250" src="doc/pic/Awesome.png">
</p>


A curated list of Multimodal learning and related area. :-)

## Contributing
Please feel free to send me [pull requests](https://github.com/njustkmg/Awesome-Multimodal-Learning/pulls) or email (yangynjust@gmail.com) to add links.
Markdown format:

```markdown
- [Paper Name](link) - Author 1 et al, `Conference Year`. [[code]](link)
```
## Change Log
## Table of Contents
- [Papers](#papers)
  - [Survey](#survey)
  - [Before](#before) - [2018](#2018) - [2019](#2019) - [2020](#2020) - [2021](#2021) - [2022](#2022)
- [Dataset](#dataset)
- [Popular Implementations](#popular-implementations)

## Papers
### Survey
* [Foundations and Recent Trends in Multimodal Machine Learning: Principles, Challenges, and Open Questions](https://www.semanticscholar.org/paper/63f93a6d9c38d656933706acfc720684470bc108) - Paul Pu Liang `ArXiv 2022`
* [Deep Vision Multimodal Learning: Methodology, Benchmark, and Trend](https://www.semanticscholar.org/paper/4cd6589b9b352311126cb2683979d8853cb724c5) - Wenhao Chai `Applied Sciences 2022,`
* [Emerging Trends of Multimodal Research in Vision and Language](https://www.semanticscholar.org/paper/0f23b548ff7b3517f028164a30c8bf186f11a1a6) - Shagun Uppal et al, ` ArXiv 2020`
### Before
* [Multimodal Machine Learning: A Survey and Taxonomy](https://www.semanticscholar.org/paper/6bc4b1376ec2812b6d752c4f6bc8d8fd0512db91) - T. Baltrušaitis et al, `IEEE Transactions on Pattern Analysis and Machine Intelligence 2017`
* [Deep Multimodal Learning: A Survey on Recent Advances and Trends](https://www.semanticscholar.org/paper/0197f278e2dedd67ec5067f47037b8cdd3ae8509) - D. Ramachandram et al，`IEEE Signal Processing Magazine 2017` 
* [Multimodal Deep Learning](https://www.semanticscholar.org/paper/80e9e3fc3670482c1fee16b2542061b779f47c4f) - Jiquan Ngiam et al,`ICML 2011`





### 2018

#### *`CVPR 2018`*

##### Image Caption

* [Neural Baby Talk](https://arxiv.org/abs/1803.09845) - Lu J et al, `CVPR 2018`. [[code]](https://github.com/jiasenlu/NeuralBabyTalk)

* [Convolutional Image Captioning](https://arxiv.org/abs/1711.09151) - Aneja J et al, `CVPR 2018`.[code](https://github.com/aditya12agd5/convcap)

* [Learning to Evaluate Image Captioning](https://arxiv.org/abs/1806.06422) - Cui Y et al, `CVPR 2018`. [[code]](https://github.com/richardaecn/cvpr18-caption-eval)

* [Discriminability Objective for Training Descriptive Captions](https://arxiv.org/abs/1803.04376) - Luo R et al, `CVPR 2018`. [[code]](https://github.com/ruotianluo/DiscCaptioning)

* [SemStyle: Learning to Generate Stylised Image Captions using Unaligned Text](https://arxiv.org/abs/1805.07030) - Mathews A et al, `CVPR 2018`.[[code]](https://github.com/computationalmedia/semstyle)

* [Bottom-Up and Top-Down Attention for Image Captioning and Visual Question Answering](https://arxiv.org/abs/1707.07998) - Anderson P et al, `CVPR 2018`. [[code]](https://github.com/peteanderson80/bottom-up-attention)

  

#### *`ECCV 2018`*

##### Image Caption

* [Rethinking the Form of Latent States in Image Captioning](http://openaccess.thecvf.com/content_ECCV_2018/papers/Bo_Dai_Rethinking_the_Form_ECCV_2018_paper.pdf) - Dai B et al, `ECCV 2018`. [[code]](https://github.com/doubledaibo/2dcaption_eccv2018)

#### *`AAAI 2018`*

##### Image Caption

* [Stack-Captioning: Coarse-to-Fine Learning for Image Captioning](https://arxiv.org/abs/1709.03376) - Gu J et al, `AAAI 2018`. [[code]](https://github.com/gujiuxiang/Stack-Captioning)

  

#### *`NeurIPS 2018`*

##### Image Caption

* [A Neural Compositional Paradigm for Image Captioning](https://arxiv.org/pdf/1810.09630.pdf) - Dai B et al, `NeurIPS 2018`.[[code]](https://github.com/ajaysub110/A-Neural-Compositional-Paradigm-for-Image-Captioning)

#### *`NAACL 2018`*

##### Image Caption

* [Defoiling Foiled Image Captions](https://arxiv.org/abs/1805.06549) - Wang J et al, `NAACL 2018`.[[code]](https://github.com/sheffieldnlp/foildataset)
* [Punny Captions: Witty Wordplay in Image Descriptions](https://arxiv.org/abs/1704.08224) - Chandrasekaran A et al, `NAACL 2018`. [[code]](https://github.com/purvaten/punny_captions)

#### *`ACL 2018`*

##### Image Caption

* [Conceptual Captions: A Cleaned, Hypernymed, Image Alt-text Dataset For Automatic Image Captioning](http://www.aclweb.org/anthology/P18-1238) - Sharma P et al, `ACL 2018`. [[code]](https://github.com/google-research-datasets/conceptual-captions)
* [Attacking visual language grounding with adversarial examples: A case study on neural image captioning](http://www.aclweb.org/anthology/P18-1241) - Chen H et al, `ACL 2018`. [[code]](https://github.com/IBM/Image-Captioning-Attack)

#### *`EMNLP 2018`*

##### Image Caption

* [simNet: Stepwise Image-Topic Merging Network for Generating Detailed and Comprehensive Image Captions](http://aclweb.org/anthology/D18-1013.pdf) - Liu et al, `EMNLP 2018`. [[code]](https://github.com/lancopku/simNet)

### 2019

#### *`CVPR 2019`*

##### Image Caption

* [Unsupervised Image Captioning](https://arxiv.org/abs/1811.10787) - Yang F et al, `CVPR 2019`. [[code]](https://github.com/fengyang0317/unsupervised_captioning)
* [Auto-Encoding Scene Graphs for Image Captioning](https://arxiv.org/abs/1812.02378) - Yang X et al, `CVPR 2019`.[[code]](https://github.com/yangxuntu/SGAE)
* [Describing like humans: on diversity in image captioning](https://arxiv.org/abs/1903.12020) - Wang Q et al, `CVPR 2019`.[[code]](https://github.com/qingzwang/DiversityMetrics)
* [Good News, Everyone! Context driven entity-aware captioning for news images](https://arxiv.org/abs/1904.01475) - Biten A F et al, `CVPR 2019`. [[code]](https://github.com/furkanbiten/GoodNews)
* [CapSal: Leveraging Captioning to Boost Semantics for Salient Object Detection](https://github.com/zhangludl/code-and-dataset-for-CapSal) - Zhang L et al, `CVPR 2019`. [[code]](https://github.com/zhangludl/code-and-dataset-for-CapSal)
* [Dense Relational Captioning: Triple-Stream Networks for Relationship-Based Captioning](https://arxiv.org/abs/1903.05942) - Kim D et al, `CVPR 2019`. [[code]](https://github.com/Dong-JinKim/DenseRelationalCaptioning)
* [Show, Control and Tell: A Framework for Generating Controllable and Grounded Captions](https://arxiv.org/abs/1811.10652v2) - Cornia M et al, `CVPR 2019`. [[code]](https://github.com/aimagelab/show-control-and-tell)
* [Exact Adversarial Attack to Image Captioning via Structured Output Learning With Latent Variables](http://openaccess.thecvf.com/content_CVPR_2019/papers/Xu_Exact_Adversarial_Attack_to_Image_Captioning_via_Structured_Output_Learning_CVPR_2019_paper.pdf) - Xu Y et al, `CVPR 2019`.[[code]](https://github.com/wubaoyuan/adversarial-attack-to-caption)

#### *`AAAI 2019`*

##### Image Caption

* [Improving Image Captioning with Conditional Generative Adversarial Nets](https://arxiv.org/abs/1805.07112) - Chen C et al, `AAAI 2019`.[[code]](https://github.com/Anjaney1999/image-captioning-seqgan)

#### *`ACL 2019`*

##### Image Caption

* [Bridging by Word: Image Grounded Vocabulary Construction for Visual Captioning](https://www.aclweb.org/anthology/P19-1652) - Fan Z et al, `ACL 2019`.[[code]](https://github.com/LibertFan/ImageCaption)

* https://github.com/ankit1khare/Show_Infer_and_Tell-CIC)

  

#### *`ICCV 2019`*

##### Image Caption

* [Attention on Attention for Image Captioning](https://arxiv.org/abs/1908.06954) - Huang L et al, `ICCV 2019`. [[code]](https://github.com/husthuaan/AoANet)
* [Learning to Collocate Neural Modules for Image Captioning](https://arxiv.org/pdf/1904.08608.pdf) - Yang X et al, `ICCV 2019`.[[code]](https://github.com/gcyzsl/cvlmn)

#### *`NeurIPS 2019`*

##### Image Caption

* [Image Captioning: Transforming Objects into Words](https://papers.nips.cc/paper/9293-image-captioning-transforming-objects-into-words) - Herdade S et al, `NeurIPS 2019`.[[code]](https://github.com/yahoo/object_relation_transformer)
* [Adaptively Aligned Image Captioning via Adaptive Attention Time](https://arxiv.org/pdf/1909.09060.pdf) - Huang L et al, `NeurIPS 2019`. [[code]](https://github.com/husthuaan/AAT)
* [Aligning Visual Regions and Textual Concepts for Semantic-Grounded Image Representations](https://papers.nips.cc/paper/8909-aligning-visual-regions-and-textual-concepts-for-semantic-grounded-image-representations.pdf) - Liu F et al, `NeurIPS 2019`. [[code]](https://github.com/fenglinliu98/MIA)

#### *`EMNLP 2019`*

##### Image Caption

* [TIGEr: Text-to-Image Grounding for Image Caption Evaluation](https://arxiv.org/pdf/1909.02050) - Jiang M et al, `EMNLP 2019`.[[code]](https://github.com/SeleenaJM/CapEval)
* [REO-Relevance, Extraness, Omission: A Fine-grained Evaluation for Image Captioning](https://arxiv.org/pdf/1909.02217) - Jiang M et al, `EMNLP 2019`.[[code]](https://github.com/SeleenaJM/CapEval)

#### *`CoNLL 2019`*

##### Image Caption

* [Compositional Generalization in Image Captioning](https://arxiv.org/pdf/1909.04402.pdf) - Nikolaus M et al, `CoNLL 2019`. [[code]](https://github.com/mitjanikolaus/compositional-image-captioning)

### 2020

#### *`AAAI 2020`*

##### Image Caption

* [MemCap: Memorizing Style Knowledge for Image Captioning](https://aaai.org/Papers/AAAI/2020GB/AAAI-ZhaoW.1402.pdf) - Zhao et al, `AAAI 2020`.[[code]](https://github.com/entalent/MemCap)
* [Unified Vision-Language Pre-Training for Image Captioning and VQA](https://www.aaai.org/Papers/AAAI/2020GB/AAAI-ZhouL.362.pdf) - Zhou L et al, `AAAI 2020`.[[code]](https://github.com/LuoweiZhou/VLP)

#### *`CVPR 2020`*

##### Image Caption

* [Say As You Wish: Fine-grained Control of Image Caption Generation with Abstract Scene Graphs](http://openaccess.thecvf.com/content_CVPR_2020/papers/Chen_Say_As_You_Wish_Fine-Grained_Control_of_Image_Caption_Generation_CVPR_2020_paper.pdf) - Chen S et al, `CVPR 2020`.[[code]](https://github.com/cshizhe/asg2cap)
* [X-Linear Attention Networks for Image Captioning](https://openaccess.thecvf.com/content_CVPR_2020/papers/Pan_X-Linear_Attention_Networks_for_Image_Captioning_CVPR_2020_paper.pdf) - Pan et al, `CVPR 2020`.[[code]](https://github.com/JDAI-CV/image-captioning)

#### *`ACL 2020`*

##### Image Caption

* [Improving Image Captioning with Better Use of Caption](https://arxiv.org/abs/2006.11807) - Shi Z et al, `ACL 2020`.[[code]](https://github.com/Gitsamshi/WeakVRD-Captioning)
* [MART: Memory-Augmented Recurrent Transformer for Coherent Video Paragraph Captioning]() - Lei J et al, `ACL 2020`.[[code]](https://github.com/jayleicn/recurrent-transformer)
* [Dense-Caption Matching and Frame-Selection Gating for Temporal Localization in VideoQA](https://arxiv.org/pdf/2005.06409) - Kim H et al, `ACL 2020`.[[code]](https://github.com/hyounghk/VideoQADenseCapFrameGate-ACL2020)

#### *`ECCV 2020`*

##### Image Caption

* [Length-Controllable Image Captioning](https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123580698.pdf) - Deng C et al, `ECCV 2020`.[[code]](https://github.com/ruotianluo/self-critical.pytorch)

* [Towards Unique and Informative Captioning of Images](https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123520613.pdf) - Wang Z et al, `ECCV 2020`.[[code]](https://github.com/princetonvisualai/SPICE-U)

* [Comprehensive Image Captioning via Scene Graph Decomposition](https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123590205.pdf) - Zhong Y et al, `ECCV 2020`.[[code]](https://github.com/yiwuzhong/sub-gc)

* [SODA: Story Oriented Dense Video Captioning Evaluation Framework](https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123510511.pdf) - Fujita S et al, `ECCV 2020`.[[code]](https://github.com/fujiso/SODA)

* [Learning to Generate Grounded Visual Captions without Localization Supervision](https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123630341.pdf) - Ma C et al, `ECCV 2020`.[[code]](https://github.com/chihyaoma/cyclical-visual-captioning)

* [Fashion Captioning: Towards Generating Accurate Descriptions with Semantic Rewards](https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123580001.pdf) - Yang X et al, `ECCV 2020`.[[code]](https://github.com/xuewyang/Fashion_Captioning)

  

#### *`EMNLP 2020`*

##### Image Caption

* [CapWAP: Image Captioning with a Purpose](https://www.aclweb.org/anthology/2020.emnlp-main.705) - Fisch A et al, `EMNLP 2020`.[[code]](https://github.com/google-research/language)
* [X-LXMERT: Paint, Caption and Answer Questions with Multi-Modal Transformers](https://www.aclweb.org/anthology/2020.emnlp-main.707) - Cho J et al, `EMNLP 2020`.[[code]](https://github.com/allenai/x-lxmert)
* [Video2Commonsense: Generating Commonsense Descriptions to Enrich Video Captioning](https://www.aclweb.org/anthology/2020.emnlp-main.61) - Fang Z et al, `EMNLP 2020`.[[code]](https://github.com/jacobswan1/Video2Commonsense)
* [Widget Captioning: Generating Natural Language Description for Mobile User Interface Elements](https://www.aclweb.org/anthology/2020.emnlp-main.443.pdf) - Li Y et al, `EMNLP 2020`.[[code]](https://github.com/google-research-datasets/widget-caption)

#### *`NeurIPS 2020`*

##### Image Caption

* [Diverse Image Captioning with Context-Object Split Latent Spaces](https://papers.nips.cc/paper/2020/hash/24bea84d52e6a1f8025e313c2ffff50a-Abstract.html) - Mahajan S et al, `NeurIPS 2020`.[[code]](https://github.com/visinf/cos-cvae)
* [RATT: Recurrent Attention to Transient Tasks for Continual Image Captioning](https://papers.nips.cc/paper/2020/hash/c2964caac096f26db222cb325aa267cb-Abstract.html) - Chiaro R et al, `NeurIPS 2020`.[[code]](https://github.com/delchiaro/RATT)

### 2021

#### AAAI 2021

- [Improving Image Captioning by Leveraging Intra- and Inter-layer Global Representation in Transformer Network](https://arxiv.org/pdf/2012.07061v1.pdf)  - Jiayi Ji et al， `AAAI 2021`[[code]](https://github.com/luo3300612/image-captioning-DLCT) 

#### CVPR 2021

- [TAP: Text-Aware Pre-training for Text-VQA and Text-Caption](https://arxiv.org/pdf/2012.04638v1.pdf) -  Zhengyuan Yang et al， `CVPR 2021`[[code]](https://github.com/microsoft/TAP) 
- [Conceptual 12M: Pushing Web-Scale Image-Text Pre-Training To Recognize Long-Tail Visual Concepts](https://arxiv.org/pdf/2102.08981v2.pdf) - Soravit Changpinyo et al,`CVPR 2021`[[code]](https://github.com/google-research-datasets/conceptual-12m) 
- [Probabilistic Embeddings for Cross-Modal Retrieval](https://arxiv.org/pdf/2101.05068v2.pdf) - Sanghyuk Chun et al, `CVPR 2021`[[code]](https://github.com/naver-ai/pcme) 
- [Towards Accurate Text-based Image Captioning with Content Diversity Exploration](https://arxiv.org/pdf/2105.03236v1.pdf) - Guanghui Xu et al, `CVPR 2021`[[code]](https://github.com/guanghuixu/AnchorCaptioner) 
- [Connecting What to Say With Where to Look by Modeling Human Attention Traces](https://arxiv.org/pdf/2105.05964v1.pdf) - Zihang Meng et al,`CVPR 2021`[[code]](https://github.com/facebookresearch/connect-caption-and-trace) 

### 2022

#### TPAMI 2022

- [Semantic Object Accuracy for Generative Text-to-Image Synthesis](https://www.aminer.cn/pub/6173f1c391e0118698c04c5f/image-quality-caption-with-attentive-and-recurrent-semantic-attractor-network)](https://arxiv.org/pdf/1910.13321v2.pdf) - Hinz Tobiaset al, `TPAMI 2022`.[[code]](https://github.com/tohinz/semantic-object-accuracy-for-generative-text-to-image-synthesis)

#### AAAI 2022

- [Nice perfume. How long did you marinate in it? Multimodal Sarcasm Explanation](https://arxiv.org/pdf/2112.04873v1.pdf) - Poorav Desai et al,`AAAI 2022`[[code]](https://github.com/lcs2-iiitd/multimodal-sarcasm-explanation-muse) 
- [MuMuQA: Multimedia Multi-Hop News Question Answering via Cross-Media Knowledge Extraction and Grounding](https://arxiv.org/pdf/2112.10728v2.pdf) - Revanth Gangi Reddy et al,`AAAI 2022`[[code]](https://github.com/blender-nlp/MuMuQA) 
- 

## Dataset
* [nocaps](https://nocaps.org/), LANG: `English`
* [MS COCO](http://cocodataset.org/), LANG: `English`.
* [Flickr 8k](https://forms.illinois.edu/sec/1713398), LANG: `English`.
* [Flickr 30k](http://shannon.cs.illinois.edu/DenotationGraph/), LANG: `English`.
* [AI Challenger](https://challenger.ai/dataset/caption), LANG: `Chinese`.
* [Visual Genome](http://visualgenome.org/), LANG: `English`.
* [SBUCaptionedPhotoDataset](http://www.cs.virginia.edu/~vicente/sbucaptions/), LANG: `English`.
* [IAPR TC-12](https://www.imageclef.org/photodata), LANG: `English, German and Spanish`.

## Popular Implementations

- [MMF](https://github.com/facebookresearch/mmf)
- [OMML](https://github.com/njustkmg/OMML)

## Licenses

To the extent possible under law, [NJUST_KMG](https://github.com/njustkmg) has waived all copyright and related or neighboring rights to this work.
